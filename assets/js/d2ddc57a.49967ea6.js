"use strict";(self.webpackChunksleepy_discord_docs=self.webpackChunksleepy_discord_docs||[]).push([[841],{963:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"voice","title":"Voice","description":"Connect to a Voice Channel","source":"@site/docs/voice.md","sourceDirName":".","slug":"/voice","permalink":"/sleepy-discord/docs/voice","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1758418732000,"frontMatter":{"title":"Voice"},"sidebar":"Docs","previous":{"title":"Requests","permalink":"/sleepy-discord/docs/requests"},"next":{"title":"Unicode and Emojis","permalink":"/sleepy-discord/docs/unicode"}}');var o=i(4848),s=i(8453);const r={title:"Voice"},c=void 0,a={},d=[{value:"Connect to a Voice Channel",id:"connect-to-a-voice-channel",level:2},{value:"Event Handling",id:"event-handling",level:2},{value:"Linking the needed libraries",id:"linking-the-needed-libraries",level:2},{value:"Sodium",id:"sodium",level:3},{value:"Opus",id:"opus",level:3},{value:"A UDP library",id:"a-udp-library",level:3},{value:"Sending Audio",id:"sending-audio",level:2},{value:"Audio Sources",id:"audio-sources",level:3},{value:"AudioPointerSource",id:"audiopointersource",level:4},{value:"AudioVectorSource",id:"audiovectorsource",level:4},{value:"Related Articles",id:"related-articles",level:4},{value:"Speak",id:"speak",level:3},{value:"Receiving Audio",id:"receiving-audio",level:2},{value:"Disconnecting",id:"disconnecting",level:2},{value:"Voice State Management",id:"voice-state-management",level:2}];function l(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"connect-to-a-voice-channel",children:"Connect to a Voice Channel"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'myClient.connectToVoiceChannel("channelID", "serverID");\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'myClient.connectToVoiceChannel(myClient.createVoiceContext("channelID", "serverID", nullptr));\n'})}),"\n",(0,o.jsxs)(n.p,{children:["There are a few ways to connect to a voice channel but the first example, calling ",(0,o.jsx)(n.code,{children:"BaseDiscordClient::connectToVoiceChannel"})," with just a ",(0,o.jsx)(n.code,{children:"channelID"})," and ",(0,o.jsx)(n.code,{children:"serverID"}),", is the simplest."]}),"\n",(0,o.jsx)(n.h2,{id:"event-handling",children:"Event Handling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'class VoiceEventHandler : public SleepyDiscord::BaseVoiceEventHandler {\r\npublic:\r\n\tVoiceEventHandler() {}\r\n\tvoid onReady(SleepyDiscord::VoiceConnection& connection) {\r\n\t\t/*Do stuff when ready to start sending audio*/\r\n\t}\r\n}\r\nVoiceEventHandler voiceEventHandler;\r\n\r\n//somewhere else in your code\r\nSleepyDiscord::VoiceContext& context = myClient.createContext("channelID", "serverID", voiceEventHandler);\r\n//or\r\ncontext.setVoiceHandler(voiceEventHandler);\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Event handling is done in a separated inherited object, a VoiceEventHandler, creating one is done by simply creating a class that inherits traits from ",(0,o.jsx)(n.code,{children:"BaseVoiceEventHandler"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["During runtime, a reference of or pointer to your VoiceEventHandler will need to be given to a ",(0,o.jsx)(n.code,{children:"VoiceContext"}),". This object should live longer then the VoiceContexts that hold the pointer to your VoiceEventHandler (VoiceContexts are destroyed after closing the connection). Not doing so will cause the client to crash."]}),"\n",(0,o.jsxs)(n.p,{children:["One useful event being the ",(0,o.jsx)(n.code,{children:"BaseVoiceEventHandler::onReady"})," event. This is called when the client has finished connecting and is ready to start sending data to one of Discord's voice servers. Some of you maybe confused as to why this is done via a callback and asking \"Shouldn't the client be ready after calling ",(0,o.jsx)(n.code,{children:"BaseDiscordClient::connectToVoiceChannel"}),"?\". This is because, there's a number of steps involved with getting ready that happen outside the function call."]}),"\n",(0,o.jsx)(n.h2,{id:"linking-the-needed-libraries",children:"Linking the needed libraries"}),"\n",(0,o.jsx)(n.p,{children:"You'll need those if you want to send or receive audio."}),"\n",(0,o.jsx)(n.h3,{id:"sodium",children:"Sodium"}),"\n",(0,o.jsxs)(n.p,{children:["Instructions on ",(0,o.jsx)(n.a,{href:"https://download.libsodium.org/doc/installation/",children:"libsodium's website"})]}),"\n",(0,o.jsx)(n.h3,{id:"opus",children:"Opus"}),"\n",(0,o.jsxs)(n.p,{children:["Download the source from ",(0,o.jsx)(n.a,{href:"https://opus-codec.org/downloads/",children:"opus's website"}),". It will contain instructions for compiling with make. You can also compile using Visual Studio from the files inside win32."]}),"\n",(0,o.jsx)(n.h3,{id:"a-udp-library",children:"A UDP library"}),"\n",(0,o.jsx)(n.p,{children:"Two options, using a custom one or you may instead use ASIO, which you should already have if you are using Websockets++ or uWebSockets."}),"\n",(0,o.jsx)(n.h2,{id:"sending-audio",children:"Sending Audio"}),"\n",(0,o.jsxs)(n.p,{children:["Once connected to a voice server with all needed library linked, we can begin sending Audio to over to Discord. To do this, create a ",(0,o.jsx)(n.code,{children:"AudioSource"}),", and then call ",(0,o.jsx)(n.code,{children:"VoiceConnection::startSpeaking"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"audio-sources",children:"Audio Sources"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'struct Music : public SleepyDiscord::AudioPointerSource {\r\n\tMusic() : SleepyDiscord::AudioPointerSource() {\r\n\t\tFile musicFile("music.raw");\r\n\t\tmusicLength = musicFile.getSize() / sizeof int16_t;\r\n\t\tmusic = musicFile.get<int16_t>();\r\n\t}\r\n\tconstexpr inline bool isOpusEncoded() { return false; } //optional, will be false by default\r\n\tvoid read(SleepyDiscord::AudioTransmissionDetails& details, int16_t*& buffer, std::size_t& length) {\r\n\t\tbuffer = &music[progress];\r\n\t\tlength = details.proposedLength() < (musicLength - progress) ? details.proposedLength() : 0;\r\n\t\tprogress += details.proposedLength();\r\n\t\t//note: set length to 0 to stop speaking\r\n\t}\r\n\tstd::size_t progress = 0;\r\n\tstd::vector<int16_t> music;\r\n\tstd::size_t musicLength;\r\n};\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:"struct SquareWave : public SleepyDiscord::AudioVectorSource {\r\n\tSquareWave() : SleepyDiscord::AudioVectorSource(), sampleOffset(0) {}\r\n\tstd::vector<int16_t> read(SleepyDiscord::AudioTransmissionDetails& details) {\r\n\t\tstd::vector<int16_t> target(details.proposedLength());\r\n\t\tfor (int16_t& sample : target) {\r\n\t\t\t\t//2000 is the volume\r\n\t\t\t\t//100 is how long half the square wave is\r\n\t\t\t\tsample = (++sampleOffset / 100) % 2 ? 2000 : -2000;\r\n\t\t}\r\n\t\treturn target;\r\n\t\t//note: return vector with a size of 0 to stop speaking\r\n\t}\r\n\tstd::size_t sampleOffset = 0;\r\n};\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Creating an AudioSource is done by creating a class that inherits a type of AudioSource and filling in the virtual function, ",(0,o.jsx)(n.code,{children:"read"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["This read function should, by default, output PCM stereo audio samples at 48000 Hz with the 2 channels interleaved.\r\nIf you aren't familiar with digital audio samples, ",(0,o.jsx)(n.a,{href:"http://manual.audacityteam.org/man/digital_audio.html",children:"here's a good article about it"}),".\r\nIn the ",(0,o.jsx)(n.code,{children:"read"})," function, you'll be given a ",(0,o.jsx)(n.code,{children:"SleepyDiscord::AudioTransmissionDetails"}),", this will give details about what audio settings to use and some other stuff like how much audio has been sent since last time.\r\nDon't worry about calling read, the library will instead call ",(0,o.jsx)(n.code,{children:"read"})," when audio data is needed while sending audio."]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"You can also send Opus encoded audio instead of PMC audio"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:"struct Music : public SleepyDiscord::AudioPointerSource {\r\n\tconstexpr inline bool isOpusEncoded() { return true; }\r\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["As of when this was written, there are two types of ",(0,o.jsx)(n.code,{children:"AudioSources"}),", pointer and vector. They differ in their read function."]}),"\n",(0,o.jsx)(n.h4,{id:"audiopointersource",children:"AudioPointerSource"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"read"})," points a pointer to a buffer of audio data and sets the length."]}),"\n",(0,o.jsx)(n.h4,{id:"audiovectorsource",children:"AudioVectorSource"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"read"})," returns a vector of audio data"]}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsx)(n.p,{children:"The sizes of audio buffers should 960. Which is .02 seconds of 48kHz stereo audio."})}),"\n",(0,o.jsx)(n.h4,{id:"related-articles",children:"Related Articles"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"http://manual.audacityteam.org/man/digital_audio.html",children:"Digital Audio Fundamentals"})}),"\n",(0,o.jsx)(n.h3,{id:"speak",children:"Speak"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:"//In your VoiceEventHandler\r\nvoid onReady(SleepyDiscord::VoiceConnection& connection) {\r\n\tconnection.startSpeaking<SquareWave>(/*Parameters to pass over to SquareWave's constructor*/);\r\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["To start speaking, call ",(0,o.jsx)(n.code,{children:"VoiceConnection::startSpeaking"})," with your AudioSource as the template parameter. To stop, send a buffer with the length of zero in your AudioSource's read function. If your AudioSource has any parameters in it's constructor, you pass them to this function. You may also use ",(0,o.jsx)(n.code,{children:"VoiceConnection::stopSpeaking"})," to stop speaking."]}),"\n",(0,o.jsx)(n.h2,{id:"receiving-audio",children:"Receiving Audio"}),"\n",(0,o.jsx)(n.p,{children:"Not implemented yet."}),"\n",(0,o.jsx)(n.h2,{id:"disconnecting",children:"Disconnecting"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:"void onEndSpeaking(SleepyDiscord::VoiceConnection& connection) {\r\n\tconnection.disconnect();\r\n}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["There are a few ways to disconnect, the simplest being calling ",(0,o.jsx)(n.code,{children:"VoiceConnection::disconnect"}),". You can also disconnect with a VoiceContext or channelID by using ",(0,o.jsx)(n.code,{children:"BaseDiscordClient::disconnectVoiceContext"})," or ",(0,o.jsx)(n.code,{children:"BaseDiscordClient:disconnectFromVoiceChannel"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"voice-state-management",children:"Voice State Management"}),"\n",(0,o.jsxs)(n.p,{children:["Using voice states, you can info about the state of users in voice chat. This data includes the joining and leaving of voice channels, deaf, mute, etc. You can keep track of voice states via the ",(0,o.jsx)(n.code,{children:"onEditVoiceState"})," event on your Discord Client and ",(0,o.jsx)(n.code,{children:"server.voiceStates"})," during the ",(0,o.jsx)(n.code,{children:"onServer"})," event."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>c});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);